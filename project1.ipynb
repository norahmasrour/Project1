{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from sqlite3 import connect\n",
        "\n",
        "print('Executing ETL Pipeline' + '\\n')\n",
        "\n",
        "\n",
        "source_type = input(\"Do you want to pull data from an API or load a local file? (Enter 'api' or 'local'): \").lower()\n",
        "\n",
        "if source_type == 'api':\n",
        "\n",
        "    url = input(\"Enter the URL of the API to pull from: \")\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        print(f\"Successfully fetched data from {url}.\")\n",
        "\n",
        "        if isinstance(data, dict) and 'data' in data:\n",
        "            df = pd.DataFrame(data['data'])\n",
        "            print(f\"API returned {len(df)} records and {len(df.columns)} columns.\")\n",
        "        else:\n",
        "            df = pd.json_normalize(data)\n",
        "            print(f\"API normalized into {len(df)} records and {len(df.columns)} columns.\")\n",
        "    else:\n",
        "        print(f\"Error fetching API data: {response.status_code} - {response.text}\")\n",
        "        df = None\n",
        "elif source_type == 'local':\n",
        "    # Local file ingestion\n",
        "    file_path = input(\"Enter the path to your local CSV or JSON file: \")\n",
        "    try:\n",
        "        if file_path.endswith('.csv'):\n",
        "            df = pd.read_csv(file_path)\n",
        "        elif file_path.endswith('.json'):\n",
        "            df = pd.read_json(file_path)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format. Use CSV or JSON.\")\n",
        "        print(f\"Successfully loaded local file '{file_path}' with {len(df)} records and {len(df.columns)} columns.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading file: {e}\")\n",
        "        df = None\n",
        "else:\n",
        "    print(\"Invalid input. Please enter 'api' or 'local'.\")\n",
        "    df = None\n",
        "\n",
        "# (add/remove columns)\n",
        "if df is not None:\n",
        "    modify_choice = input(\"Do you want to add or remove columns? (Enter 'add', 'remove', or 'none'): \").lower()\n",
        "    if modify_choice == 'add':\n",
        "        new_col_name = input(\"Enter the name of the new column: \")\n",
        "        default_value = input(f\"Enter the default value for the new column '{new_col_name}': \")\n",
        "        df[new_col_name] = default_value\n",
        "        print(f\"Added column '{new_col_name}' with default value '{default_value}'.\")\n",
        "    elif modify_choice == 'remove':\n",
        "        col_to_remove = input(\"Enter the name of the column to remove: \")\n",
        "        if col_to_remove in df.columns:\n",
        "            df.drop(columns=[col_to_remove], inplace=True)\n",
        "            print(f\"Removed column '{col_to_remove}'.\")\n",
        "        else:\n",
        "            print(f\"Column '{col_to_remove}' does not exist in the dataset.\")\n",
        "\n",
        "# Save the dataset as CSV or SQL\n",
        "if df is not None:\n",
        "    save_choice = input(\"Do you want to save the dataset as CSV or SQL? (Enter 'csv' or 'sql'): \").lower()\n",
        "    if save_choice == 'csv':\n",
        "        output_csv = 'output_data.csv'\n",
        "        df.to_csv(output_csv, index=False)\n",
        "        print(f\"Data saved to {output_csv} with {len(df)} records and {len(df.columns)} columns.\")\n",
        "    elif save_choice == 'sql':\n",
        "        output_db = 'output_data.db'\n",
        "        conn = connect(output_db)\n",
        "        df.to_sql('data', conn, if_exists='replace', index=False)\n",
        "        conn.close()\n",
        "        print(f\"Data saved to SQL database '{output_db}' with {len(df)} records and {len(df.columns)} columns.\")\n",
        "    else:\n",
        "        print(\"Invalid input. Please enter 'csv' or 'sql'.\")\n",
        "\n",
        "print(\"ETL Pipeline execution completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ll9zat-f_kTp",
        "outputId": "7d0420fd-2998-4e8f-849d-32d14e1a7bbf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing ETL Pipeline\n",
            "\n",
            "Do you want to pull data from an API or load a local file? (Enter 'api' or 'local'): local\n",
            "Enter the path to your local CSV or JSON file: Global_Map_Full_Data_data.csv\n",
            "Successfully loaded local file 'Global_Map_Full_Data_data.csv' with 593 records and 8 columns.\n",
            "Do you want to add or remove columns? (Enter 'add', 'remove', or 'none'): none\n",
            "Do you want to save the dataset as CSV or SQL? (Enter 'csv' or 'sql'): sql\n",
            "Data saved to SQL database 'output_data.db' with 593 records and 8 columns.\n",
            "ETL Pipeline execution completed.\n"
          ]
        }
      ]
    }
  ]
}